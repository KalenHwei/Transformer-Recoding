{
 "cells": [
  {
   "cell_type": "raw",
   "id": "39356e56-1a10-40f8-8e42-0c4f9e04a5d4",
   "metadata": {},
   "source": [
    "Embedding是什么？怎么用？\n",
    "\n",
    "1，Embedding 是一种将离散数据（如单词、字符、类别等）转换为连续向量表示的方法。这种方法在自然语言处理（NLP）和机器学习的其他领域中广泛应用。具体来说，Embedding 层主要用于以下几个方面：\n",
    "\n",
    "1） 转换离散数据到连续空间\n",
    "    离散数据（如单词）通常以整数索引表示。Embedding 层通过查找表的方式将这些索引映射到高维连续向量空间中，使得模型能够更好地处理这些数据。\n",
    "\n",
    "2） 捕捉语义信息\n",
    "    在自然语言处理中，Embedding 层能够捕捉到单词之间的语义关系。通过训练，具有相似语义的单词会在嵌入空间中靠得更近。\n",
    "    比如，“猫”和“狗”可能会有相似的嵌入向量，而“猫”和“汽车”的嵌入向量则会相差较大。\n",
    "\n",
    "3） 降维和特征提取\n",
    "    相比于使用独热编码（one-hot encoding），Embedding 层能够显著降低维度，并且可以提取更多的特征信息。独热编码通常会导致维度非常高\n",
    "    而 Embedding 层则能够在较低维度上表示更多的语义信息。\n",
    "\n",
    "4） 处理类别数据\n",
    "    在推荐系统和分类问题中，类别数据（如用户ID、商品ID等）也可以通过 Embedding 层进行处理，从而将这些类别数据映射到连续向量空间中，便于后续模型的训练。\n",
    "\n",
    "总的来说，Embedding提供了一种比独热编码更好的对输入对象编码的方法。\n",
    "\n",
    "2，nn.Embedding()的用法\n",
    "    nn.Embedding(num_embeddings, embedding_dim)\n",
    "    其中，num_embeddings：词汇表的大小，即嵌入字典的行数。mbedding_dim：每个词的嵌入向量的维度。\n",
    "    词汇表的大小和嵌入向量维度依照经验执行\n",
    "    词汇表大小：如果是一个大型MLP模型，词汇表大小一般为100000；对于分类任务，例如一共有10000个客户，则设置为10000\n",
    "    嵌入向量维度：一般设置为20-500,取决于任务大小\n",
    "    例如，现有一串输入“我爱学习”，定义“我”=1， “爱”=2， “学习“=3\n",
    "    因此，输入为input = torch.tensor([1, 2, 3])\n",
    "    现在定义一个Embedding层：embedding = nn.Embedding(10, 5)\n",
    "    则输出为output = embedding(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939f5e77-6298-487e-bc8d-3b35b08f533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4713, -1.2182, -0.8645,  0.5597,  0.4086],\n",
      "        [-0.5292,  0.1039,  1.1292, -0.9744, -0.8612],\n",
      "        [-0.4023,  0.2278, -0.0304,  0.7337,  0.5182],\n",
      "        [-0.4297, -0.2095, -0.4040, -0.8297, -0.2001],\n",
      "        [ 0.2489,  0.3468,  0.4028,  0.1965, -0.8521],\n",
      "        [-0.7692,  0.2284, -0.3751, -1.8834,  1.6906],\n",
      "        [-1.7145,  0.0438, -1.9743,  0.3906,  1.2447],\n",
      "        [ 0.5273, -0.0397, -1.4526, -1.0032,  0.2694],\n",
      "        [ 0.0846,  0.2693, -0.8422, -0.5228,  0.1128],\n",
      "        [-0.2328,  1.3979, -1.1034,  0.7489,  0.6000]], requires_grad=True)\n",
      "tensor([[-0.5292,  0.1039,  1.1292, -0.9744, -0.8612],\n",
      "        [-0.4023,  0.2278, -0.0304,  0.7337,  0.5182],\n",
      "        [-0.4297, -0.2095, -0.4040, -0.8297, -0.2001]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2.nn.Embedding()的用法示例\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1）初始化一个词汇表大小为10，嵌入维度为5的Embedding层\n",
    "embedding = nn.Embedding(10, 5)\n",
    "print(embedding.weight)#之所以不全为0，是因为初始化是随机初始化，否则会出现对称性问题\n",
    "\n",
    "# 初始化词汇索引\n",
    "input = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 2）获取嵌入向量\n",
    "output = embedding(input)\n",
    "print(output)#输出为embedding.weight的二三四行，因为是根据input得到的输出\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
