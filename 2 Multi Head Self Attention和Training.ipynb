{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974ff876-2205-4fd4-9920-8c3a407b238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0fd68b-871c-4893-8b8c-193edf3dfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.shape = (batch_size, N, dmodel)\n",
    "\n",
    "class MHselfattention(nn.Module):\n",
    "    def __init__(self, dk, dv, num_heads, dmodel):\n",
    "        super(MHselfattention, self).__init__()\n",
    "        self.wq = nn.Parameter(torch.randn((num_heads, dmodel, dk)), requires_grad=True)\n",
    "        self.wk = nn.Parameter(torch.randn((num_heads, dmodel, dk)), requires_grad=True)\n",
    "        self.wv = nn.Parameter(torch.randn((num_heads, dmodel, dv)), requires_grad=True)\n",
    "        self.output_linear = nn.Linear(num_heads * dv, dmodel)\n",
    "        self.scale = math.sqrt(dk)\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dmodel = x.size()\n",
    "        \n",
    "        q_splits = torch.stack([torch.matmul(x, self.wq[i]) for i in range(self.num_heads)], dim=0)# wq[i]形状为(dmodel, dk),因此matmul(x, wq)形状为(batch_size, N, dk)\n",
    "        k_splits = torch.stack([torch.matmul(x, self.wk[i]) for i in range(self.num_heads)], dim=0)# 再在第一维度stack起来，因此最后的形状是(num_heads, batch_size, N, dk)\n",
    "        v_splits = torch.stack([torch.matmul(x, self.wv[i]) for i in range(self.num_heads)], dim=0)\n",
    "\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            q = q_splits[i] # 每个q的shape是(batch_size, N, dk)\n",
    "            k = k_splits[i]# 每个k的shape是(batch_size, N, dk)\n",
    "            v = v_splits[i]# 每个v的shape是(batch_size, N, dv)\n",
    "\n",
    "            k_trans = k.transpose(-2, -1) # 变成(batch_size, dk, N)\n",
    "            scores = torch.matmul(q, k_trans) / self.scale # shape是(batch_size, N, N)\n",
    "            weights = torch.softmax(scores, dim=1) # shape是(batch_size, N, N)\n",
    "            output = torch.matmul(weights, v) # shape是(batch_size, N, dv)\n",
    "            outputs.append(output) # outputs在经历num_heads个循环之后，本质上是一列长度为num_heads的向量列表，其中的每个元素是output，大小为(batch_size, N, dv)\n",
    "            \n",
    "        concat_outputs = torch.cat(outputs, dim=-1) # 按最后一个维度cat起outputs，因此shape=(batch_size, N, num_heads*dv)\n",
    "        final_output = self.output_linear(concat_outputs) # 定义了nn.Linear(num_heads * dv, dmodel)，因此(batch_size, N, num_heads*dv)的最后一维变成了dmodel\n",
    "        \n",
    "        return final_output # 输出，即shape为(batch_size, N, dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2e2ed7-8a02-46e2-b8c7-93461f408cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "\n",
    "def test_mhselfattention():\n",
    "    # 定义超参数\n",
    "    batch_size = 10\n",
    "    N = 6\n",
    "    dmodel = 512\n",
    "    dk = 512\n",
    "    dv = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # 创建输入张量\n",
    "    x = torch.randn(batch_size, N, dmodel)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = MHselfattention(dk=dk, dv=dv, num_heads=num_heads, dmodel=dmodel)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    \n",
    "    # 检查输出形状\n",
    "    expected_shape = (batch_size, N, dmodel)\n",
    "    \n",
    "    assert output.shape == expected_shape, f\"Expected output shape {expected_shape}, but got {output.shape}\"\n",
    "    \n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# 运行测试\n",
    "test_mhselfattention()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad27338d-df67-4aec-82fa-6f98fb6e4a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 19.1212\n",
      "Epoch [20/100], Loss: 8.1841\n",
      "Epoch [30/100], Loss: 1.9286\n",
      "Epoch [40/100], Loss: 0.9113\n",
      "Epoch [50/100], Loss: 0.3228\n",
      "Epoch [60/100], Loss: 0.1145\n",
      "Epoch [70/100], Loss: 0.0410\n",
      "Epoch [80/100], Loss: 0.0151\n",
      "Epoch [90/100], Loss: 0.0055\n",
      "Epoch [100/100], Loss: 0.0020\n",
      "Final Loss: 0.0020\n",
      "Total Time (steps): 100\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "#定义超参\n",
    "batch_size = 10\n",
    "N = 6\n",
    "dmodel = 512\n",
    "dk = 512\n",
    "dv = 512\n",
    "num_heads = 8\n",
    "num_epoths = 100\n",
    "\n",
    "#定义模型、损失函数、优化器\n",
    "model = MHselfattention(dk=dk, dv=dv, num_heads=num_heads, dmodel=dmodel)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#定义数据\n",
    "x = torch.randn(batch_size, N, dmodel)\n",
    "y_true = torch.randn(batch_size, N, dmodel)\n",
    "\n",
    "def train(x, y_true, num_epochs, model, optimizer, loss_function):\n",
    "    time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设定模型为训练模式\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        output = model(x) # 前向传播\n",
    "        # 确保输出的维度与 y_true 匹配\n",
    "        if output.shape != y_true.shape:\n",
    "            raise ValueError(f\"Output shape {output.shape} does not match target shape {y_true.shape}\")\n",
    "\n",
    "        loss = loss_function(output, y_true)# 计算损失\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "        time += 1 # 增加时间计数\n",
    "        # 打印损失\n",
    "        if (epoch + 1) % 10 == 0:  # 每 10 个 epoch 打印一次损失\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return loss, time\n",
    "\n",
    "final_loss, final_time = train(x, y_true, num_epoths, model=model, optimizer=optimizer, loss_function=loss_function)\n",
    "\n",
    "# 打印最终结果\n",
    "print(f'Final Loss: {final_loss.item():.4f}')\n",
    "print(f'Total Time (steps): {final_time}')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5bcc9-4746-4f84-bae6-c2ff1e96b8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
