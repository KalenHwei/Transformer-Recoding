{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4fb135-b216-4fc5-9224-37894aa5728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930bbce8-0789-4a80-9cb6-ac583c816ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义self attention类，涉及参数dk，dv，dmodel\n",
    "class selfattention(nn.Module):\n",
    "    def __init__(self, dk, dv, dmodel):\n",
    "        super(selfattention, self).__init__()\n",
    "        self.wk = nn.Parameter(torch.randn((dmodel, dk)), requires_grad=True)\n",
    "        self.wq = nn.Parameter(torch.randn((dmodel, dk)), requires_grad=True)\n",
    "        self.wv = nn.Parameter(torch.randn((dmodel, dv)), requires_grad=True)\n",
    "        self.scale = math.sqrt(dk)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = torch.matmul(x, self.wk)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "        \n",
    "        k_trans = torch.transpose(k, -2, -1)\n",
    "        \n",
    "        scores = torch.matmul(q, k_trans) / self.scale\n",
    "        \n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "        output = torch.matmul(weights, v)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b240c00b-d64d-4e55-94dd-94ab87203609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1506, -3.4778,  3.5662,  0.2548,  2.0066, -5.2641, -2.3709, -1.4959,\n",
      "         -2.6784,  2.5706],\n",
      "        [-2.3301, -3.6530,  1.0091, -0.9456,  0.0206, -0.5263, -2.5090,  3.6106,\n",
      "          1.2469, -0.0290],\n",
      "        [ 2.5769,  3.6930,  0.7355,  2.1224,  2.1392,  2.2069,  4.6420, -0.5704,\n",
      "          3.7105,  6.4722],\n",
      "        [ 2.0587, -0.8596,  0.0837,  4.1209, -0.8531,  2.6658,  4.3049, -2.0848,\n",
      "         -4.6817, -2.2809],\n",
      "        [-0.1594, -0.3190,  4.1963,  2.9322,  0.2026,  0.0194, -0.4789, -4.2381,\n",
      "         -1.3737,  2.5007],\n",
      "        [ 2.1397,  4.5830,  0.3462,  2.3870,  1.8430, -5.0613,  0.2383,  4.5549,\n",
      "         -4.4616, -4.4314]], grad_fn=<MmBackward0>) torch.Size([6, 10])\n"
     ]
    }
   ],
   "source": [
    "#测试self attention类是否正常工作\n",
    "dmodel = 10\n",
    "dk = 10\n",
    "dv = 10\n",
    "\n",
    "sa = selfattention(dk, dv, dmodel)\n",
    "x = torch.randn(6, dmodel)  # 假设输入序列长度为 10\n",
    "output = sa(x)\n",
    "print(output, output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3eee9bd-4b30-4040-98f2-5a8e5615fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练函数\n",
    "model = selfattention(dk, dv, dmodel)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "x = torch.randn(6, dmodel)\n",
    "y_true = torch.randn(6, dv)\n",
    "\n",
    "def train(x, y_true, num_epochs, model, optimizer, loss_function):\n",
    "    time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设定模型为训练模式\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        output = model(x)  # 前向传播\n",
    "        loss = loss_function(output, y_true)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "        time += 1\n",
    "        # 打印损失\n",
    "        if (epoch + 1) % 10 == 0:  # 每 10 个 epoch 打印一次损失\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return loss, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c2d0b7-b93b-406f-868c-9b127a221019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 10.5026\n",
      "Epoch [20/100], Loss: 9.3150\n",
      "Epoch [30/100], Loss: 8.1951\n",
      "Epoch [40/100], Loss: 7.4871\n",
      "Epoch [50/100], Loss: 7.1009\n",
      "Epoch [60/100], Loss: 6.8334\n",
      "Epoch [70/100], Loss: 6.6070\n",
      "Epoch [80/100], Loss: 6.3987\n",
      "Epoch [90/100], Loss: 6.2013\n",
      "Epoch [100/100], Loss: 6.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(6.0118, grad_fn=<MseLossBackward0>), 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试训练函数\n",
    "\n",
    "# 假设训练数据\n",
    "x = torch.randn(6, dmodel)  # 输入数据\n",
    "y_true = torch.randn(6, dv)  # 目标数据\n",
    "\n",
    "# 训练模型\n",
    "train(x, y_true, num_epochs=100, model=model, optimizer=optimizer, loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4d67022-2485-471f-95cc-8871ca79e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#带有batch的self attention，基本和普通SA一样\n",
    "\n",
    "class batchselfattention(nn.Module):\n",
    "    def __init__(self, dk, dv, dmodel):\n",
    "        super(batchselfattention, self).__init__()\n",
    "        self.wq = nn.Parameter(torch.randn(dmodel, dk), requires_grad=True)\n",
    "        self.wk = nn.Parameter(torch.randn(dmodel, dk), requires_grad=True)\n",
    "        self.wv = nn.Parameter(torch.randn(dmodel, dv), requires_grad=True)\n",
    "        self.scales = math.sqrt(dk)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dmodel = x.size()\n",
    "        \n",
    "        q = torch.matmul(x, self.wq)\n",
    "        k = torch.matmul(x, self.wk)\n",
    "        v = torch.matmul(x, self.wv)\n",
    "        \n",
    "        k_trans = k.transpose(-2, -1)\n",
    "        \n",
    "        scores = torch.matmul(q, k_trans)/self.scales\n",
    "        \n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        output = torch.matmul(weights, v)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ddfa61e-63e1-41a4-902e-5a5a9a1ccb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "#测试带batch的SA\n",
    "\n",
    "dk = 768\n",
    "dv = 768\n",
    "dmodel = 768\n",
    "batch_size = 10\n",
    "N = 20\n",
    "\n",
    "model = batchselfattention(dk, dv, dmodel)\n",
    "x = torch.randn(batch_size, N, dmodel)\n",
    "output = model(x)\n",
    "print(np.shape(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
