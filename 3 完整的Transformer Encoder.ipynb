{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17419e9b-4e00-4c49-a8f4-a76350d4fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30122abb-a3cb-4e4f-8e6f-0a5e1bfe8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义Multi_head_self_attention、前向神经网络、Transformer Encoder层，最后封装成完整的Transformer\n",
    "\n",
    "class MHselfattention(nn.Module):\n",
    "    def __init__(self, dk, dv, num_heads, dmodel):\n",
    "        super(MHselfattention, self).__init__()\n",
    "        self.wq = nn.Parameter(torch.randn((num_heads, dmodel, dk)), requires_grad=True)\n",
    "        self.wk = nn.Parameter(torch.randn((num_heads, dmodel, dk)), requires_grad=True)\n",
    "        self.wv = nn.Parameter(torch.randn((num_heads, dmodel, dv)), requires_grad=True)\n",
    "        self.output_linear = nn.Linear(num_heads * dv, dmodel)\n",
    "        self.scale = math.sqrt(dk)\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dmodel = x.size()\n",
    "        q_splits = torch.stack([torch.matmul(x, self.wq[i]) for i in range(self.num_heads)], dim=0)\n",
    "        k_splits = torch.stack([torch.matmul(x, self.wk[i]) for i in range(self.num_heads)], dim=0)\n",
    "        v_splits = torch.stack([torch.matmul(x, self.wv[i]) for i in range(self.num_heads)], dim=0)\n",
    "        outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            q = q_splits[i]\n",
    "            k = k_splits[i]\n",
    "            v = v_splits[i]\n",
    "            k_trans = k.transpose(-2, -1)\n",
    "            scores = torch.matmul(q, k_trans) / self.scale\n",
    "            weights = torch.softmax(scores, dim=1)\n",
    "            output = torch.matmul(weights, v)\n",
    "            outputs.append(output)\n",
    "        concat_outputs = torch.cat(outputs, dim=-1)\n",
    "        final_output = self.output_linear(concat_outputs)\n",
    "        return final_output\n",
    "\n",
    "# multi_head_self_attn = MHselfattention(dk, dv, mum_heads, dmodel)\n",
    "# x.shape = (batch_size, N, dmodel=512)\n",
    "# output_multi_head_self_attn = multi_head_self_attn(x)\n",
    "# output_multi_head_self_attn.shape = (batch_size, N, dmodel=512)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dmodel, dff, dropout_rate=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dmodel, dff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear2 = nn.Linear(dff, dmodel)\n",
    "    \n",
    "    def forward(self, x): #输入x的shape=(batch_size, N, dmodel=512)\n",
    "        x = self.linear1(x) # shape=(batch_size, N, dff)\n",
    "        x = self.relu(x) # 激活函数不变\n",
    "        x = self.dropout(x) # dropout不变\n",
    "        x = self.linear2(x) # shape=(batch_size, N, dmodel)\n",
    "        return x # 也就是最后输出的x的shape=(batch_size, N, dmodel)\n",
    "\n",
    "# MHselfattention的output作为FeedForward的input\n",
    "# feed_forwawrd = FeedForward(dmodel=512, dff, dropout_rate=0.1)\n",
    "# x.shape = (batch_size, N, dmodel=512)\n",
    "# output_feedforward = feed_forward(x)\n",
    "# 也就是最后输出的x的shape=(batch_size, N, dmodel=512)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, dk, dv, num_heads, dmodel, dff, dropout_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.mha = MHselfattention(dk, dv, num_heads, dmodel)\n",
    "        self.ffn = FeedForward(dmodel, dff, dropout_rate)\n",
    "        self.layernorm1 = nn.LayerNorm(dmodel)\n",
    "        self.layernorm2 = nn.LayerNorm(dmodel)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output = self.mha(x) # 先做一次 Multi-Head Attention\n",
    "        x = self.layernorm1(x + attn_output) # 然后Add & Norm标准化\n",
    "        ffn_output = self.ffn(x)# 前向传播一次，即经过一次定义的FeedForward\n",
    "        x = self.layernorm2(x + ffn_output) # 第二次Add & Norm标准化\n",
    "        return x # 最后的输出shape还是(batch_size, N, dmodel=512)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, dk, dv, num_heads, dmodel, dff, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(dk, dv, num_heads, dmodel, dff, dropout_rate) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9540ae-83d4-497f-9280-acde21f53bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([5, 10, 16])\n"
     ]
    }
   ],
   "source": [
    "# 调用示例\n",
    "\n",
    "# 定义超参\n",
    "batch_size = 5\n",
    "seq_len = 10\n",
    "dmodel = 16\n",
    "dk = 8\n",
    "dv = 8\n",
    "num_heads = 4\n",
    "dff = 64\n",
    "num_layers = 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# 创建输入数据\n",
    "x = torch.randn(batch_size, seq_len, dmodel)\n",
    "\n",
    "# 初始化 Transformer Encoder\n",
    "encoder = TransformerEncoder(num_layers=num_layers, dk=dk, dv=dv, num_heads=num_heads, dmodel=dmodel, dff=dff, dropout_rate=dropout_rate)\n",
    "\n",
    "# 前向传播\n",
    "output = encoder(x)\n",
    "print(f'Output shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ed40c7-82ff-4b1e-bb04-557afb38a409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y_true, num_epochs, model, optimizer, loss_function, device):\n",
    "    \"\"\"\n",
    "    训练 Transformer Encoder 模型\n",
    "    \n",
    "    参数：\n",
    "    - x (Tensor): 输入张量，形状为 (batch_size, N, dmodel)\n",
    "    - y_true (Tensor): 目标张量，形状为 (batch_size, N, dmodel)\n",
    "    - num_epochs (int): 训练的轮数\n",
    "    - model (nn.Module): Transformer Encoder 模型\n",
    "    - optimizer (torch.optim.Optimizer): 优化器\n",
    "    - loss_function (nn.Module): 损失函数\n",
    "    \n",
    "    返回：\n",
    "    - final_loss (Tensor): 最终损失值\n",
    "    - time (int): 总的训练步骤数\n",
    "    \"\"\"\n",
    "    model.to(device) # 将模型和训练数据部署到其他设备上\n",
    "    x = x.to(device) # 同上\n",
    "    y_true = y_true.to(device) # 同上\n",
    "    \n",
    "    time = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设定模型为训练模式\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        output = model(x) # 前向传播\n",
    "        # 确保输出的维度与 y_true 匹配\n",
    "        if output.shape != y_true.shape:\n",
    "            raise ValueError(f\"Output shape {output.shape} does not match target shape {y_true.shape}\")\n",
    "\n",
    "        loss = loss_function(output, y_true) # 计算损失\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "        time += 1\n",
    "        \n",
    "        # 打印损失\n",
    "        if (epoch + 1) % 10 == 0:  # 每 10 个 epoch 打印一次损失\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return loss, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b68a514-f4cf-4ad2-95d1-c0670ef52ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.6358\n",
      "Epoch [20/100], Loss: 1.3809\n",
      "Epoch [30/100], Loss: 1.2020\n",
      "Epoch [40/100], Loss: 1.0895\n",
      "Epoch [50/100], Loss: 1.0303\n",
      "Epoch [60/100], Loss: 1.0071\n",
      "Epoch [70/100], Loss: 1.0012\n",
      "Epoch [80/100], Loss: 1.0016\n",
      "Epoch [90/100], Loss: 1.0010\n",
      "Epoch [100/100], Loss: 1.0010\n",
      "Final Loss: 1.0010\n",
      "Total Time (steps): 100\n"
     ]
    }
   ],
   "source": [
    "# 定义超参、损失函数、优化器，调用训练函数进行训练\n",
    "\n",
    "# 定义模型参数\n",
    "batch_size = 20\n",
    "N = 10\n",
    "dmodel = 512\n",
    "dk = 512\n",
    "dv = 512\n",
    "num_heads = 8\n",
    "dff = 64\n",
    "num_layers = 6\n",
    "dropout_rate = 0.1\n",
    "num_epochs = 100\n",
    "\n",
    "#定义训练设备，我用的是macbook，没有cuda就只能用mps加速\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# 创建输入数据和目标数据\n",
    "x = torch.randn(batch_size, N, dmodel)  # 假设的输入数据\n",
    "y_true = torch.randn(batch_size, N, dmodel)  # 假设的目标数据\n",
    "\n",
    "# 初始化 Transformer Encoder 模型\n",
    "model = TransformerEncoder(num_layers=num_layers, dk=dk, dv=dv, num_heads=num_heads, dmodel=dmodel, dff=dff, dropout_rate=dropout_rate)\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# 运行训练\n",
    "final_loss, final_time = train(x, y_true, num_epochs, model=model, optimizer=optimizer, loss_function=loss_function, device=device)\n",
    "\n",
    "# 打印最终结果\n",
    "print(f'Final Loss: {final_loss.item():.4f}')\n",
    "print(f'Total Time (steps): {final_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
