{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e48789-082e-4084-b052-64ddabd4d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1287d4c-4844-432f-8a4c-4b76af72a6cf",
   "metadata": {},
   "source": [
    "# 一，词嵌入编码\n",
    "# 考虑source sentense 和 target sentense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03c40a15-806c-4b71-ada9-a1d0a3c8085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1，首先规定批量训练大小为2\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9090760b-db7d-4f7b-ba10-411adceb7662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3], dtype=torch.int32)\n",
      "tensor([3, 2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 2，初始化source和target词向量的长度，并转化为torch.int32格式）\n",
    "# torch.randint(2, 4, (batch_size,))表示生成2到4之间的整数，元素个数为batch_size，并以元祖形式(batch_size,)保存\n",
    "# 加逗号，否则会识别成batch_size本身\n",
    "# 并转化成torch.int32格式，因为默认是int64，会多占用一般内存。我们不需要这么高精度\n",
    "src_len = torch.randint(2, 4, (batch_size,)).to(torch.int32)\n",
    "tgt_len = torch.randint(2, 4, (batch_size,)).to(torch.int32)\n",
    "print(src_len) # 打印输出结果检查\n",
    "print(tgt_len) # 同上\n",
    "\n",
    "# 也就是说，src_len表示的是batch_size个句子，每个句子的长度为其元素\n",
    "# 例如tensor([2, 3])，就是有两个句子，第一个句子长度为2，第二个句子长度为3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5615971-4700-4648-a6f6-a382655ef2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([6, 7]), tensor([4, 8, 8])]\n",
      "[tensor([8, 5, 3]), tensor([4, 5])]\n"
     ]
    }
   ],
   "source": [
    "# 3，根据src_len和tgt_len的长度生成对应的序列\n",
    "# 其中，[torch.randint(1, 8, (l,))的2和10表示最小单词量为1，最大单词量为8\n",
    "# 用for循环遍历src_len的元素l，该元素作为初始化src_seq的大小形状\n",
    "# 例如，src_len为tensor([2,4]),则src_seq的第一个元素就是tensor(a, b, 2)\n",
    "# 最后用列表存储\n",
    "src_seq = [torch.randint(1, 8, (l,)) for l in src_len]\n",
    "tgt_seq = [torch.randint(1, 8, (l,)) for l in tgt_len]\n",
    "print(src_seq)\n",
    "print(tgt_seq)\n",
    "\n",
    "# 例如src_seq = [torch.randint(1, 10, (l,)) for l in src_len]\n",
    "# 由于src_len = tensor([2, 3])\n",
    "# 因此for l in src_len循环的就是每个元素的具体值\n",
    "# 对于第一个元素2，则生成一个从1到8随机取两个数的向量tensor([6, 7])\n",
    "# 对于第二个元素3，则生成一个从1到8随机取三个数的向量tensor([4, 8, 8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88fee2f9-8904-4280-9373-a325e31fe9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([4, 1, 0, 0, 0]), tensor([7, 6, 2, 0, 0])]\n",
      "[tensor([1, 3, 7, 0, 0]), tensor([6, 5, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "# 4，由于每个len的长度可能不一样，因此在这里要做一个padding填充\n",
    "# 使得所有向量的形状大小一样，方便后续统一计算\n",
    "# 因此引入统一变量，重新定义序列seq\n",
    "\n",
    "# 设置单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "# 定义每个padding后序列的最大长度\n",
    "max_src_seg_len = 5\n",
    "max_tgt_seg_len = 5\n",
    "\n",
    "# 用F.pad填充。填充的输入是(torch.randint(1, max_num_src_words, (l,))\n",
    "# 左边无填充所以是0\n",
    "# 右边为自定义的序列最大长度减去遍历到的l\n",
    "# ，因此对于，src_len的第一个元素是2，则最大长度-2=填充3个0\n",
    "src_seq = [F.pad(torch.randint(1, max_num_src_words, (l,)), (0, max_src_seg_len - l)) for l in src_len]\n",
    "tgt_seq = [F.pad(torch.randint(1, max_num_tgt_words, (l,)), (0, max_tgt_seg_len - l)) for l in tgt_len]\n",
    "\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bee065f-3ce2-439a-b8fe-8bb3451e6c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 1, 0, 0, 0],\n",
      "        [3, 6, 2, 0, 0]])\n",
      "tensor([[7, 3, 7, 0, 0],\n",
      "        [6, 6, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 由于上述输出的src_seq和tgt_seq为两个分开的tensor向量，因此希望将向量改写成矩阵，以方便后续计算\n",
    "# 继续改写src_seq和tgt_seq，使用stack函数\n",
    "# 其中，tensor.stack(输入，dim）\n",
    "# 输入一般为tensor向量或矩阵，dim为在哪个维度上连接，dim为0时则额外创建一个维度连接；dim为1时则行连接，等等\n",
    "# 例如输入为一维向量时， dim=0会创建一个二维空间，在第二维度把向量放进去\n",
    "# 输入为二维矩阵时，dim=0会在三维上放进去\n",
    "# 以此类推\n",
    "\n",
    "src_seq = [F.pad(torch.randint(1, max_num_src_words, (l,)), (0, max_src_seg_len - l)) for l in src_len]\n",
    "src_seq = torch.stack(src_seq, 0)\n",
    "print(src_seq)\n",
    "\n",
    "tgt_seq = [F.pad(torch.randint(1, max_num_tgt_words, (l,)), (0, max_tgt_seg_len - l)) for l in tgt_len]\n",
    "tgt_seq = torch.stack(tgt_seq, 0)\n",
    "print(tgt_seq)\n",
    "\n",
    "#最终得到tensor矩阵形式的src_seq和tgt_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0fc980b-53d4-448e-a6f6-b9a4a901582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876, -0.9850],\n",
      "        [-0.4791, -0.3004,  2.0029, -0.7475, -0.1174,  0.0927,  1.1969, -0.4423],\n",
      "        [ 0.5184,  2.1956,  1.7249,  0.8338, -0.0674,  0.8243, -1.3604,  1.1947],\n",
      "        [-2.2040, -0.1672,  0.9562,  1.5547,  1.4049,  0.3914,  0.3390, -2.5726],\n",
      "        [ 1.6854,  0.8938, -0.6191, -0.4602, -0.8093,  0.8120,  1.0549,  0.4549],\n",
      "        [ 1.4196,  0.7809,  0.4749,  1.4905,  0.4259, -0.4340,  0.6600, -1.2104],\n",
      "        [ 1.0450, -0.7187, -0.1449,  0.8069,  0.0235, -0.2626, -0.1451,  0.4248],\n",
      "        [-1.1366, -2.0651,  1.4856,  0.2466,  0.3595,  1.2130, -0.8386,  1.1096],\n",
      "        [ 0.3650,  0.0262, -0.0271, -0.0484,  1.0287,  0.1781, -1.7947, -0.8245]],\n",
      "       requires_grad=True)\n",
      "tensor([[6, 1, 0, 0, 0],\n",
      "        [3, 6, 2, 0, 0]])\n",
      "tensor([[[ 1.0450, -0.7187, -0.1449,  0.8069,  0.0235, -0.2626, -0.1451,\n",
      "           0.4248],\n",
      "         [-0.4791, -0.3004,  2.0029, -0.7475, -0.1174,  0.0927,  1.1969,\n",
      "          -0.4423],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850]],\n",
      "\n",
      "        [[-2.2040, -0.1672,  0.9562,  1.5547,  1.4049,  0.3914,  0.3390,\n",
      "          -2.5726],\n",
      "         [ 1.0450, -0.7187, -0.1449,  0.8069,  0.0235, -0.2626, -0.1451,\n",
      "           0.4248],\n",
      "         [ 0.5184,  2.1956,  1.7249,  0.8338, -0.0674,  0.8243, -1.3604,\n",
      "           1.1947],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850]]], grad_fn=<EmbeddingBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.8878,  1.6520, -1.6887,  0.0689,  0.9259, -0.9087, -0.8695,  0.6564],\n",
      "        [-1.1020,  0.7160,  0.1927, -2.3649,  0.7130,  0.2779,  0.1362,  1.0530],\n",
      "        [ 2.4146, -1.4819, -2.0701,  0.2550, -1.4768, -0.6186,  1.0957, -0.0756],\n",
      "        [-1.3597, -0.3753, -0.5666,  0.5644,  0.9265,  0.4403, -0.2246,  0.0835],\n",
      "        [ 0.8430, -0.0030,  0.4633, -0.1151,  0.4088,  0.4319, -0.7134,  0.5812],\n",
      "        [-0.1703, -0.1902,  0.2494, -0.5607,  1.0057, -0.6990, -0.2969, -0.2041],\n",
      "        [ 0.4266, -1.0980, -0.2656,  0.8481,  0.6343,  0.5841,  1.5289, -0.9460],\n",
      "        [-1.4294, -1.1829,  1.1677, -1.1397, -0.5709, -0.4260, -0.7580, -1.9149],\n",
      "        [ 0.5910, -0.9494,  0.4109,  1.0202, -0.4313, -0.0254,  0.5938,  0.9404]],\n",
      "       requires_grad=True)\n",
      "tensor([[7, 3, 7, 0, 0],\n",
      "        [6, 6, 0, 0, 0]])\n",
      "tensor([[[-1.1366, -2.0651,  1.4856,  0.2466,  0.3595,  1.2130, -0.8386,\n",
      "           1.1096],\n",
      "         [-2.2040, -0.1672,  0.9562,  1.5547,  1.4049,  0.3914,  0.3390,\n",
      "          -2.5726],\n",
      "         [-1.1366, -2.0651,  1.4856,  0.2466,  0.3595,  1.2130, -0.8386,\n",
      "           1.1096],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850]],\n",
      "\n",
      "        [[ 1.0450, -0.7187, -0.1449,  0.8069,  0.0235, -0.2626, -0.1451,\n",
      "           0.4248],\n",
      "         [ 1.0450, -0.7187, -0.1449,  0.8069,  0.0235, -0.2626, -0.1451,\n",
      "           0.4248],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850],\n",
      "         [ 0.3305, -0.7157, -1.5897,  1.1334, -0.2211, -0.7553,  1.1876,\n",
      "          -0.9850]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 5，构造embedding矩阵\n",
    "# embedding本质就是一种特殊的独热编码\n",
    "# embedding的具体用法参考外面的embedding学习的ipynb文件\n",
    "\n",
    "model_dim = 8#根据transformer模型的超参来的，原文512，这里简化为8。这里对应原文的dmodel\n",
    "\n",
    "\n",
    "# 用nn.Embedding初始化embedding矩阵，其大小为最大词库+1（因为padding的东西为0）乘model_dim\n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim)\n",
    "\n",
    "# 然后将src_seq作为输入传入embedding table里，得到关于每个单词（也就是每行）的编码信息\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "\n",
    "print(src_embedding_table.weight) # 打印src_seq中的每个单词（即原文的1、2、...、8以及pad得来的0共9个元素的关于model_dim的词embedding编码向量)\n",
    "print(src_seq) # 上面的src_embedding_table.weight素材来源\n",
    "print(src_embedding) # 用src_seq中涉及的每个单词看每个单词的词embedding编码向量\n",
    "\n",
    "#同理，tgt也一样处理\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim)\n",
    "tgt_embedding = src_embedding_table(tgt_seq)\n",
    "\n",
    "\n",
    "print(tgt_embedding_table.weight)\n",
    "print(tgt_seq)\n",
    "print(tgt_embedding)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e948f4d3-8ebf-4ae4-8cd1-94bd9e760e35",
   "metadata": {},
   "source": [
    "二、构造positional embedding(不太会)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a12cc5d-9049-48b6-9b0c-c319d9f6dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
      "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00]]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# 定义序列最大长度\n",
    "max_position_len = 5\n",
    "\n",
    "# 构造position embedding,对应原文的pos\n",
    "pos_mat = torch.arange(max_position_len).reshape((-1, 1))\n",
    "\n",
    "# 构造i_mat，对应原文的i\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1,-1))/model_dim)\n",
    "\n",
    "# 构造一个pe_embedding_table，先初始化为0\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "\n",
    "#pe_embedding_table的偶数列为\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "\n",
    "#pe_embedding_table的奇数列为\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "#输出pe_embedding_table\n",
    "print(pe_embedding_table)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad = False)\n",
    "\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "print(src_pe_embedding)\n",
    "print(tgt_pe_embedding)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46ab8edd-6697-4424-9089-087a331e5cfa",
   "metadata": {},
   "source": [
    "三 Encoder的 self attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57d116-328c-4efd-9afa-7c79e834b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Attention(Q, K, V)\n",
    "# 我们希望被mask的值放在softmax里变为负无穷，因此在softmax的结果中体现为0\n",
    "# 因此我们需要构造一个矩阵，通过乘上这个矩阵，我们需要mask掉的元素变为负无穷，不需要mask的元素不变\n",
    "# mask的size为(batch_size, max_src_len, max_src_len)，值为1和负无穷\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
